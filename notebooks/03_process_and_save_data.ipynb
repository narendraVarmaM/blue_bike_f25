{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f415a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82214207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9a6523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from src.config import RAW_DATA_DIR, PROCESSED_DATA_DIR, TRANSFORMED_DATA_DIR\n",
    "\n",
    "# Set up directories\n",
    "RAW_DIR = RAW_DATA_DIR\n",
    "PROCESSED_DIR = PROCESSED_DATA_DIR\n",
    "TRANSFORMED_DIR = TRANSFORMED_DATA_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b25be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 572 stations\n",
      "  station_id station_code                          station_name municipality  \\\n",
      "0        461       L32001    Railroad Lot and Minuteman Bikeway    Arlington   \n",
      "1        462       L32002       Linwood St at Minuteman Bikeway    Arlington   \n",
      "2        480       L32005  Thorndike Field at Minuteman Bikeway    Arlington   \n",
      "3        464       L32003                Mass Ave at Grafton St    Arlington   \n",
      "4        465       L32004                Broadway at Grafton St    Arlington   \n",
      "\n",
      "         Lat       Long  lat_round  lon_round  \n",
      "0  42.416065 -71.153366       42.4      -71.2  \n",
      "1  42.409354 -71.149065       42.4      -71.1  \n",
      "2  42.400168 -71.144570       42.4      -71.1  \n",
      "3  42.407261 -71.143821       42.4      -71.1  \n",
      "4  42.409942 -71.140093       42.4      -71.1  \n"
     ]
    }
   ],
   "source": [
    "def load_station_mapping() -> pd.DataFrame:\n",
    "    \"\"\"Load station data and create rounded coordinates for weather mapping\"\"\"\n",
    "    STATIONS_FILE = RAW_DIR / \"-External-_Bluebikes_Station_List.xlsx\"\n",
    "    stations = pd.read_excel(STATIONS_FILE, header=1)\n",
    "    \n",
    "    # Create rounded coordinates for weather mapping\n",
    "    stations[\"lat_round\"] = stations[\"Lat\"].round(1)\n",
    "    stations[\"lon_round\"] = stations[\"Long\"].round(1)\n",
    "    \n",
    "    # Clean column names and select relevant columns\n",
    "    stations = stations.rename(columns={\n",
    "        \"Station ID (to match to historic system data)\": \"station_id\",\n",
    "        \"Number\": \"station_code\",\n",
    "        \"NAME\": \"station_name\",\n",
    "        \"Municipality\": \"municipality\"\n",
    "    })\n",
    "    \n",
    "    return stations[[\"station_id\", \"station_code\", \"station_name\", \"municipality\", \n",
    "                    \"Lat\", \"Long\", \"lat_round\", \"lon_round\"]]\n",
    "\n",
    "# Load stations\n",
    "stations = load_station_mapping()\n",
    "print(f\"Loaded {len(stations)} stations\")\n",
    "print(stations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f36574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: holidays in /opt/anaconda3/envs/py313/lib/python3.13/site-packages (0.82)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/envs/py313/lib/python3.13/site-packages (from holidays) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/py313/lib/python3.13/site-packages (from python-dateutil->holidays) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4655d9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rides_2024_10.parquet\n",
      "Loading rides_2024_11.parquet\n",
      "Loading rides_2024_12.parquet\n",
      "Loading rides_2025_01.parquet\n",
      "Loading rides_2025_02.parquet\n",
      "Loading rides_2025_03.parquet\n",
      "Loading rides_2025_04.parquet\n",
      "Loading rides_2025_05.parquet\n",
      "Loading rides_2025_06.parquet\n",
      "Loading rides_2025_07.parquet\n",
      "Loading rides_2025_08.parquet\n",
      "Loading rides_2025_09.parquet\n",
      "Before filling: 1,638,389 records\n",
      "After filling: 5,187,319 records\n",
      "Loaded 5,187,319 hourly ride records\n",
      "Time range: 2024-09-30 22:00:00 to 2025-09-30 23:00:00\n",
      "Stations: 592\n",
      "Columns: ['hour', 'start_station_id', 'ride_count']\n",
      "\n",
      "Sample of filled rides data:\n",
      "                 hour start_station_id  ride_count\n",
      "0 2024-09-30 22:00:00           A32000           0\n",
      "1 2024-09-30 23:00:00           A32000           0\n",
      "2 2024-10-01 00:00:00           A32000           0\n",
      "3 2024-10-01 01:00:00           A32000           0\n",
      "4 2024-10-01 02:00:00           A32000           0\n",
      "5 2024-10-01 03:00:00           A32000           0\n",
      "6 2024-10-01 04:00:00           A32000           0\n",
      "7 2024-10-01 05:00:00           A32000           0\n",
      "8 2024-10-01 06:00:00           A32000           0\n",
      "9 2024-10-01 07:00:00           A32000           3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Optional\n",
    "import holidays\n",
    "\n",
    "def fill_missing_rides_full_range(df, hour_col, location_col, rides_col):\n",
    "    \"\"\"\n",
    "    Fills in missing rides for all hours in the range and all unique locations.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with columns [hour_col, location_col, rides_col]\n",
    "    - hour_col: Name of the column containing hourly timestamps\n",
    "    - location_col: Name of the column containing location IDs\n",
    "    - rides_col: Name of the column containing ride counts\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with missing hours and locations filled in with 0 rides\n",
    "    \"\"\"\n",
    "    # Ensure the hour column is in datetime format\n",
    "    df[hour_col] = pd.to_datetime(df[hour_col])\n",
    "\n",
    "    # Get the full range of hours (from min to max) with hourly frequency\n",
    "    full_hours = pd.date_range(\n",
    "        start=df[hour_col].min(),\n",
    "        end=df[hour_col].max(),\n",
    "        freq=\"h\"\n",
    "    )\n",
    "\n",
    "    # Get all unique location IDs\n",
    "    all_locations = df[location_col].unique()\n",
    "\n",
    "    # Create a DataFrame with all combinations of hours and locations\n",
    "    full_combinations = pd.DataFrame(\n",
    "        [(hour, location) for hour in full_hours for location in all_locations],\n",
    "        columns=[hour_col, location_col]\n",
    "    )\n",
    "\n",
    "    # Merge the original DataFrame with the full combinations DataFrame\n",
    "    merged_df = pd.merge(full_combinations, df, on=[hour_col, location_col], how='left')\n",
    "\n",
    "    # Fill missing rides with 0\n",
    "    merged_df[rides_col] = merged_df[rides_col].fillna(0).astype(int)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def is_us_holiday(date):\n",
    "    \"\"\"Check if a date is a US holiday\"\"\"\n",
    "    us_holidays = holidays.UnitedStates()\n",
    "    return date in us_holidays\n",
    "\n",
    "def load_and_aggregate_rides(years: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"Load ride data and aggregate to hourly counts per station with complete time series\"\"\"\n",
    "    all_rides = []\n",
    "    \n",
    "    for year in years:\n",
    "        # Determine months based on year\n",
    "        if year == 2024:\n",
    "            months = range(10, 13)  # october to december for 2024\n",
    "        else:\n",
    "            months = range(1, 10)  # through september for 2025\n",
    "            \n",
    "        for month in months:\n",
    "            file_path = RAW_DIR / f\"rides_{year}_{month:02}.parquet\"\n",
    "            if file_path.exists():\n",
    "                print(f\"Loading {file_path.name}\")\n",
    "                \n",
    "                # Read rides data\n",
    "                rides = pd.read_parquet(file_path)\n",
    "                \n",
    "                # Convert timestamps and filter valid rides\n",
    "                rides[\"started_at\"] = pd.to_datetime(rides[\"started_at\"], errors=\"coerce\")\n",
    "                rides = rides.dropna(subset=[\"started_at\", \"start_station_id\"])\n",
    "                \n",
    "                # Round to hour and aggregate\n",
    "                rides[\"hour\"] = rides[\"started_at\"].dt.floor(\"H\")\n",
    "                hourly_rides = rides.groupby([\"hour\", \"start_station_id\"]).size().reset_index(name=\"ride_count\")\n",
    "                \n",
    "                all_rides.append(hourly_rides)\n",
    "    \n",
    "    if not all_rides:\n",
    "        raise ValueError(\"No ride data found!\")\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_rides = pd.concat(all_rides, ignore_index=True)\n",
    "    \n",
    "    print(f\"Before filling: {len(combined_rides):,} records\")\n",
    "    \n",
    "    # Apply the fill_missing_rides_full_range function\n",
    "    combined_rides = fill_missing_rides_full_range(\n",
    "        combined_rides, \n",
    "        hour_col=\"hour\", \n",
    "        location_col=\"start_station_id\", \n",
    "        rides_col=\"ride_count\"\n",
    "    )\n",
    "    \n",
    "    print(f\"After filling: {len(combined_rides):,} records\")\n",
    "    \n",
    "    # Add ONLY the required time features\n",
    "    #combined_rides[\"is_weekend\"] = combined_rides[\"hour\"].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Add US holiday feature\n",
    "    #combined_rides[\"is_us_holiday\"] = combined_rides[\"hour\"].dt.date.apply(is_us_holiday).astype(int)\n",
    "    \n",
    "    # Sort by station and hour for consistency\n",
    "    combined_rides = combined_rides.sort_values([\"start_station_id\", \"hour\"]).reset_index(drop=True)\n",
    "    \n",
    "    return combined_rides\n",
    "\n",
    "\n",
    "# Load rides data with complete time series filling\n",
    "rides_df = load_and_aggregate_rides([2024, 2025])\n",
    "print(f\"Loaded {len(rides_df):,} hourly ride records\")\n",
    "print(f\"Time range: {rides_df['hour'].min()} to {rides_df['hour'].max()}\")\n",
    "print(f\"Stations: {rides_df['start_station_id'].nunique()}\")\n",
    "print(f\"Columns: {list(rides_df.columns)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample of filled rides data:\")\n",
    "print(rides_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98baedf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weather_2024_10.parquet\n",
      "Loading weather_2024_11.parquet\n",
      "Loading weather_2024_12.parquet\n",
      "Loading weather_2025_01.parquet\n",
      "Loading weather_2025_02.parquet\n",
      "Loading weather_2025_03.parquet\n",
      "Loading weather_2025_04.parquet\n",
      "Loading weather_2025_05.parquet\n",
      "Loading weather_2025_06.parquet\n",
      "Loading weather_2025_07.parquet\n",
      "Loading weather_2025_08.parquet\n",
      "Loading weather_2025_09.parquet\n",
      "Weather columns: ['hour', 'lat_round', 'lon_round', 'temp', 'humidity', 'pressure', 'wind_speed', 'wind_deg', 'precipitation']\n",
      "Loaded 61,250 hourly weather records\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_weather(years: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"Load and process weather data - keep all raw weather metrics without grading\"\"\"\n",
    "    all_weather = []\n",
    "    \n",
    "    for year in years:\n",
    "        if year == 2024:\n",
    "            months = range(10, 13)\n",
    "        else:\n",
    "            months = range(1, 10)\n",
    "            \n",
    "        for month in months:\n",
    "            file_path = RAW_DIR / f\"weather_{year}_{month:02}.parquet\"\n",
    "            if file_path.exists():\n",
    "                print(f\"Loading {file_path.name}\")\n",
    "                weather = pd.read_parquet(file_path)\n",
    "                \n",
    "                # Ensure proper datetime and numeric types\n",
    "                weather[\"timestamp\"] = pd.to_datetime(weather[\"timestamp\"], errors=\"coerce\")\n",
    "                weather = weather.dropna(subset=[\"timestamp\"])\n",
    "                \n",
    "                # Round to hour and take mean if multiple records per hour\n",
    "                weather[\"hour\"] = weather[\"timestamp\"].dt.floor(\"H\")\n",
    "                weather_agg = weather.groupby([\"hour\", \"lat_round\", \"lon_round\"]).agg({\n",
    "                    \"temp\": \"mean\",\n",
    "                    \"humidity\": \"mean\", \n",
    "                    \"pressure\": \"mean\",\n",
    "                    \"wind_speed\": \"mean\",\n",
    "                    \"wind_deg\": \"mean\",\n",
    "                    \"precipitation\": \"mean\"\n",
    "                }).reset_index()\n",
    "                \n",
    "                all_weather.append(weather_agg)\n",
    "    \n",
    "    if not all_weather:\n",
    "        raise ValueError(\"No weather data found!\")\n",
    "    \n",
    "    combined_weather = pd.concat(all_weather, ignore_index=True)\n",
    "    \n",
    "    # Remove weather grade creation - just return raw weather metrics\n",
    "    print(f\"Weather columns: {list(combined_weather.columns)}\")\n",
    "    \n",
    "    return combined_weather\n",
    "\n",
    "# Load weather data\n",
    "weather_df = load_and_process_weather([2024, 2025])\n",
    "print(f\"Loaded {len(weather_df):,} hourly weather records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22376b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a66168b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rides hour timezone: None\n",
      "Weather hour timezone: None\n",
      "Rides with coords: 5,187,319 records\n",
      "Available weather coordinates: 7 unique pairs\n",
      "Warning: 236574 records missing coordinates after station merge\n",
      "Merged dataset: 4,950,745 records\n",
      "Available weather columns: ['temp', 'humidity', 'pressure', 'wind_speed', 'wind_deg', 'precipitation']\n",
      "  temp: filled 0 missing values\n",
      "  humidity: filled 0 missing values\n",
      "  pressure: filled 0 missing values\n",
      "  wind_speed: filled 0 missing values\n",
      "  wind_deg: filled 0 missing values\n",
      "  precipitation: filled 0 missing values\n",
      "  temp: filled remaining 0 with station mean\n",
      "  humidity: filled remaining 0 with station mean\n",
      "  pressure: filled remaining 0 with station mean\n",
      "  wind_speed: filled remaining 0 with station mean\n",
      "  wind_deg: filled remaining 0 with station mean\n",
      "  precipitation: filled remaining 0 with station mean\n",
      "Final merged dataset: 4,950,745 records\n",
      "Columns: ['hour', 'start_station_id', 'ride_count', 'lat_round', 'lon_round', 'temp', 'humidity', 'pressure', 'wind_speed', 'wind_deg', 'precipitation']\n"
     ]
    }
   ],
   "source": [
    "def merge_rides_weather(rides_df: pd.DataFrame, weather_df: pd.DataFrame, stations: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Merge rides data with weather data using station coordinates\"\"\"\n",
    "    \n",
    "    # First, ensure both hour columns are timezone-naive for proper merging\n",
    "    rides_df = rides_df.copy()\n",
    "    weather_df = weather_df.copy()\n",
    "    \n",
    "    # Remove timezone from weather hour if it exists\n",
    "    if rides_df['hour'].dt.tz is not None:\n",
    "        rides_df['hour'] = rides_df['hour'].dt.tz_localize(None)\n",
    "    if weather_df['hour'].dt.tz is not None:\n",
    "        weather_df['hour'] = weather_df['hour'].dt.tz_localize(None)\n",
    "    \n",
    "    print(f\"Rides hour timezone: {rides_df['hour'].dt.tz}\")\n",
    "    print(f\"Weather hour timezone: {weather_df['hour'].dt.tz}\")\n",
    "    \n",
    "    # Merge rides with station information to get coordinates\n",
    "    rides_with_coords = rides_df.merge(\n",
    "        stations[[\"station_code\", \"lat_round\", \"lon_round\"]].rename(columns={\"station_code\": \"start_station_id\"}),\n",
    "        left_on=\"start_station_id\", \n",
    "        right_on=\"start_station_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Rides with coords: {len(rides_with_coords):,} records\")\n",
    "    print(f\"Available weather coordinates: {weather_df[['lat_round', 'lon_round']].drop_duplicates().shape[0]} unique pairs\")\n",
    "    \n",
    "    # Check for missing coordinates after merge\n",
    "    missing_coords = rides_with_coords['lat_round'].isnull().sum()\n",
    "    if missing_coords > 0:\n",
    "        print(f\"Warning: {missing_coords} records missing coordinates after station merge\")\n",
    "        rides_with_coords = rides_with_coords.dropna(subset=['lat_round', 'lon_round'])\n",
    "    \n",
    "    # Merge with weather data\n",
    "    merged_df = rides_with_coords.merge(\n",
    "        weather_df,\n",
    "        left_on=[\"hour\", \"lat_round\", \"lon_round\"],\n",
    "        right_on=[\"hour\", \"lat_round\", \"lon_round\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Merged dataset: {len(merged_df):,} records\")\n",
    "    \n",
    "    # Define weather columns (remove weather_grade since we don't want it)\n",
    "    weather_cols = [\"temp\", \"humidity\", \"pressure\", \"wind_speed\", \"wind_deg\", \"precipitation\"]\n",
    "    \n",
    "    # Check which weather columns actually exist in the merged data\n",
    "    available_weather_cols = [col for col in weather_cols if col in merged_df.columns]\n",
    "    print(f\"Available weather columns: {available_weather_cols}\")\n",
    "    \n",
    "    # Sort by station and time for proper filling\n",
    "    merged_df = merged_df.sort_values([\"start_station_id\", \"hour\"])\n",
    "    \n",
    "    # Fill missing values by station group only for available weather columns\n",
    "    for col in available_weather_cols:\n",
    "        missing_before = merged_df[col].isnull().sum()\n",
    "        merged_df[col] = merged_df.groupby(\"start_station_id\")[col].transform(\n",
    "            lambda x: x.fillna(method='ffill').fillna(method='bfill')\n",
    "        )\n",
    "        missing_after = merged_df[col].isnull().sum()\n",
    "        print(f\"  {col}: filled {missing_before - missing_after} missing values\")\n",
    "    \n",
    "    # For any remaining missing values, use station-specific mean\n",
    "    for col in available_weather_cols:\n",
    "        remaining_missing = merged_df[col].isnull().sum()\n",
    "        if remaining_missing > 0:\n",
    "            merged_df[col] = merged_df.groupby(\"start_station_id\")[col].transform(\n",
    "                lambda x: x.fillna(x.mean())\n",
    "            )\n",
    "            final_missing = merged_df[col].isnull().sum()\n",
    "            print(f\"  {col}: filled remaining {remaining_missing - final_missing} with station mean\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Merge the datasets\n",
    "merged_data = merge_rides_weather(rides_df, weather_df, stations)\n",
    "print(f\"Final merged dataset: {len(merged_data):,} records\")\n",
    "print(f\"Columns: {list(merged_data.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
