{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7d9065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Blue Bikes Data Transformation Pipeline\n",
      "==================================================\n",
      "Loading processed data...\n",
      "Loaded data: 5,187,319 records\n",
      "Columns: ['hour', 'start_station_id', 'ride_count', 'is_weekend', 'is_us_holiday', 'lat_round', 'lon_round', 'temp', 'humidity', 'pressure', 'wind_speed', 'wind_deg', 'precipitation']\n",
      "\n",
      "==================================================\n",
      "TRANSFORMING DATA WITH SLIDING WINDOW\n",
      "==================================================\n",
      "Starting sliding window transformation...\n",
      "Rides window size: 672 (28 days)\n",
      "Weather window size: 3 hours\n",
      "Step size: 1\n",
      "Number of stations after filtering: 50\n",
      "Processing 50 stations...\n",
      "Processed 10 stations...\n",
      "Processed 20 stations...\n",
      "Processed 30 stations...\n",
      "Processed 40 stations...\n",
      "Processed 50 stations...\n",
      "Transformation complete!\n",
      "Final dataset: 404584 records\n",
      "Number of features: 681\n",
      "  - Ride features: 672\n",
      "  - Weather features: 9\n",
      "\n",
      "==================================================\n",
      "STORING TRANSFORMED DATA\n",
      "==================================================\n",
      "Stored transformed data to: ../data/transformed/transformed_features_and_target_top50.parquet\n",
      "Dataset shape: (404584, 684)\n",
      "\n",
      "==================================================\n",
      "ANALYSIS\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "TRANSFORMED DATA ANALYSIS\n",
      "============================================================\n",
      "Total records: 404,584\n",
      "Number of stations: 50\n",
      "Time range: 2024-10-28 22:00:00 to 2025-09-30 23:00:00\n",
      "\n",
      "Feature breakdown:\n",
      "  Ride features: 672\n",
      "  Weather features: 9\n",
      "  Total features: 681\n",
      "Missing values: 3641256\n",
      "\n",
      "Target statistics:\n",
      "  Min: 0\n",
      "  Max: 98\n",
      "  Mean: 3.49\n",
      "  Std: 4.74\n",
      "\n",
      "Sample of transformed data:\n",
      "   target start_station_id                hour  rides_t-672  rides_t-671  \\\n",
      "0      13           A32002 2024-10-28 22:00:00          0.0          0.0   \n",
      "1       3           A32002 2024-10-28 23:00:00          0.0          0.0   \n",
      "2       1           A32002 2024-10-29 00:00:00          0.0          3.0   \n",
      "3       1           A32002 2024-10-29 01:00:00          3.0          1.0   \n",
      "4       0           A32002 2024-10-29 02:00:00          1.0          0.0   \n",
      "\n",
      "   rides_t-670  rides_t-669  rides_t-668  \n",
      "0          0.0          3.0          1.0  \n",
      "1          3.0          1.0          0.0  \n",
      "2          1.0          0.0          0.0  \n",
      "3          0.0          0.0          0.0  \n",
      "4          0.0          0.0          3.0  \n",
      "\n",
      "ðŸŽ‰ TRANSFORMATION COMPLETED SUCCESSFULLY!\n",
      "The dataset is now ready for machine learning model training.\n",
      "\n",
      "Code execution completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up directories\n",
    "RAW_DIR = Path(\"../data/raw\")\n",
    "PROCESSED_DIR = Path(\"../data/processed\")\n",
    "TRANSFORMED_DIR = Path(\"../data/transformed\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [PROCESSED_DIR, TRANSFORMED_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def transform_ts_data_into_features_and_target(\n",
    "    df: pd.DataFrame,\n",
    "    feature_col: str = \"ride_count\",\n",
    "    weather_cols: List[str] = None,\n",
    "    rides_window_size: int = 672,  # 28 days * 24 hours = 672\n",
    "    weather_window_size: int = 3,   # 3 hours of weather data\n",
    "    step_size: int = 1,\n",
    "    top_n_stations: int = 50\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform time series data into features and target using sliding window approach.\n",
    "    Based on the reference code pattern.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with time series data\n",
    "        feature_col (str): Main feature column for rides\n",
    "        weather_cols (List[str]): Weather columns to include\n",
    "        rides_window_size (int): Window size for rides (28 days = 672 hours)\n",
    "        weather_window_size (int): Window size for weather data\n",
    "        step_size (int): Step size for sliding window\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed DataFrame with features and target\n",
    "    \"\"\"\n",
    "    \n",
    "    if weather_cols is None:\n",
    "        weather_cols = [\"temp\", \"wind_speed\", \"precipitation\"]\n",
    "    \n",
    "    print(\"Starting sliding window transformation...\")\n",
    "    print(f\"Rides window size: {rides_window_size} (28 days)\")\n",
    "    print(f\"Weather window size: {weather_window_size} hours\")\n",
    "    print(f\"Step size: {step_size}\")\n",
    "    \n",
    "    station_totals = (\n",
    "        df.groupby(\"start_station_id\")[feature_col]\n",
    "        .sum()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    top_stations = station_totals.head(top_n_stations).index\n",
    "    df = df[df[\"start_station_id\"].isin(top_stations)].copy()\n",
    "\n",
    "    print(f\"Number of stations after filtering: {len(top_stations)}\")\n",
    "\n",
    "    # Get all unique station IDs\n",
    "    station_ids = df[\"start_station_id\"].unique()\n",
    "    print(f\"Processing {len(station_ids)} stations...\")\n",
    "    \n",
    "    # List to store transformed data for each station\n",
    "    all_transformed_data = []\n",
    "    \n",
    "    for station_id in station_ids:\n",
    "        try:\n",
    "            # Filter data for current station and sort by hour\n",
    "            station_data = df[df[\"start_station_id\"] == station_id].copy()\n",
    "            station_data = station_data.sort_values(\"hour\").reset_index(drop=True)\n",
    "            \n",
    "            # Skip if not enough data\n",
    "            if len(station_data) <= rides_window_size:\n",
    "                print(f\"Skipping station {station_id}: insufficient data ({len(station_data)} records)\")\n",
    "                continue\n",
    "            \n",
    "            # Extract values as arrays\n",
    "            ride_values = station_data[feature_col].values\n",
    "            hour_values = station_data[\"hour\"].values\n",
    "            \n",
    "            # Extract weather values\n",
    "            weather_values = {}\n",
    "            for col in weather_cols:\n",
    "                if col in station_data.columns:\n",
    "                    weather_values[col] = station_data[col].values\n",
    "            \n",
    "            # Create sliding windows\n",
    "            rows = []\n",
    "            for i in range(0, len(ride_values) - rides_window_size, step_size):\n",
    "                # Rides features (last 28 days = 672 hours)\n",
    "                ride_features = ride_values[i:i + rides_window_size]\n",
    "                \n",
    "                # Weather features (last 3 hours for each weather variable)\n",
    "                weather_features = []\n",
    "                for col in weather_cols:\n",
    "                    if col in weather_values:\n",
    "                        # Get the last 3 hours of weather data (aligned with the end of rides window)\n",
    "                        weather_data = weather_values[col][i + rides_window_size - weather_window_size:i + rides_window_size]\n",
    "                        weather_features.extend(weather_data)\n",
    "                \n",
    "                # Target (next hour's ride count)\n",
    "                target = ride_values[i + rides_window_size]\n",
    "                \n",
    "                # Timestamp for the target\n",
    "                target_hour = hour_values[i + rides_window_size]\n",
    "                \n",
    "                # Station ID\n",
    "                station_id_val = station_id\n",
    "                \n",
    "                # Combine all features and metadata\n",
    "                all_features = np.concatenate([ride_features, np.array(weather_features)])\n",
    "                row = np.append(all_features, [target, station_id_val, target_hour])\n",
    "                rows.append(row)\n",
    "            \n",
    "            # Create column names\n",
    "            ride_feature_cols = [f\"rides_t-{rides_window_size - j}\" for j in range(rides_window_size)]\n",
    "            \n",
    "            weather_feature_cols = []\n",
    "            for col in weather_cols:\n",
    "                if col in weather_values:\n",
    "                    weather_feature_cols.extend([f\"{col}_t-{weather_window_size - k}\" for k in range(weather_window_size)])\n",
    "            \n",
    "            all_feature_cols = ride_feature_cols + weather_feature_cols\n",
    "            all_columns = all_feature_cols + [\"target\",\"start_station_id\", \"hour\"]\n",
    "            \n",
    "            # Create DataFrame for this station\n",
    "            if rows:  # Check if we have any rows\n",
    "                station_transformed = pd.DataFrame(rows, columns=all_columns)\n",
    "                all_transformed_data.append(station_transformed)\n",
    "                \n",
    "                if len(all_transformed_data) % 10 == 0:  # Print progress every 10 stations\n",
    "                    print(f\"Processed {len(all_transformed_data)} stations...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing station {station_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_transformed_data:\n",
    "        raise ValueError(\"No data could be transformed. Check window sizes and data availability.\")\n",
    "    \n",
    "    # Combine all stations\n",
    "    final_df = pd.concat(all_transformed_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"Transformation complete!\")\n",
    "    print(f\"Final dataset: {len(final_df)} records\")\n",
    "    print(f\"Number of features: {len(all_feature_cols)}\")\n",
    "    print(f\"  - Ride features: {len(ride_feature_cols)}\")\n",
    "    print(f\"  - Weather features: {len(weather_feature_cols)}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def store_transformed_data(transformed_df: pd.DataFrame, filename: str = \"transformed_features_and_target_top50.parquet\"):\n",
    "    \"\"\"Store the transformed features and target data\"\"\"\n",
    "    \n",
    "    output_path = TRANSFORMED_DIR / filename\n",
    "    transformed_df.to_parquet(output_path, index=False)\n",
    "    \n",
    "    print(f\"Stored transformed data to: {output_path}\")\n",
    "    print(f\"Dataset shape: {transformed_df.shape}\")\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "def analyze_transformed_data(transformed_df: pd.DataFrame):\n",
    "    \"\"\"Analyze the transformed dataset\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRANSFORMED DATA ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Total records: {len(transformed_df):,}\")\n",
    "    print(f\"Number of stations: {transformed_df['start_station_id'].nunique()}\")\n",
    "    print(f\"Time range: {transformed_df['hour'].min()} to {transformed_df['hour'].max()}\")\n",
    "    \n",
    "    # Count feature types\n",
    "    ride_features = [col for col in transformed_df.columns if 'rides_t-' in col]\n",
    "    weather_features = [col for col in transformed_df.columns if any(w in col for w in \n",
    "                        ['temp', 'wind_speed', 'precipitation'])]\n",
    "    \n",
    "    print(f\"\\nFeature breakdown:\")\n",
    "    print(f\"  Ride features: {len(ride_features)}\")\n",
    "    print(f\"  Weather features: {len(weather_features)}\")\n",
    "    print(f\"  Total features: {len(transformed_df.columns) - 3}\")  # Excluding target, station_id, hour\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = transformed_df.isnull().sum().sum()\n",
    "    print(f\"Missing values: {missing_values}\")\n",
    "    \n",
    "    # Target statistics\n",
    "    print(f\"\\nTarget statistics:\")\n",
    "    print(f\"  Min: {transformed_df['target'].min()}\")\n",
    "    print(f\"  Max: {transformed_df['target'].max()}\")\n",
    "    print(f\"  Mean: {transformed_df['target'].mean():.2f}\")\n",
    "    print(f\"  Std: {transformed_df['target'].std():.2f}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nSample of transformed data:\")\n",
    "    sample_cols = ['target', 'start_station_id', 'hour'] + transformed_df.columns[:5].tolist()\n",
    "    print(transformed_df[sample_cols].head())\n",
    "\n",
    "# MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Blue Bikes Data Transformation Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Load your merged data (adjust the path as needed)\n",
    "        processed_file = PROCESSED_DIR / \"rides_weather_combined.parquet\"\n",
    "        \n",
    "        if processed_file.exists():\n",
    "            print(\"Loading processed data...\")\n",
    "            merged_data = pd.read_parquet(processed_file)\n",
    "            print(f\"Loaded data: {len(merged_data):,} records\")\n",
    "            print(f\"Columns: {list(merged_data.columns)}\")\n",
    "        else:\n",
    "            # If no processed file exists, use a sample (you'll need to replace this with your actual data loading)\n",
    "            print(\"Processed file not found. Please ensure you have run the data merging step first.\")\n",
    "            print(\"Creating sample data for demonstration...\")\n",
    "            \n",
    "            # Sample data structure - REPLACE THIS WITH YOUR ACTUAL DATA LOADING\n",
    "            dates = pd.date_range(\"2024-01-01\", \"2024-12-31\", freq=\"H\")\n",
    "            sample_data = []\n",
    "            for station_id in [\"A32000\", \"A32001\", \"A32002\"]:\n",
    "                for date in dates:\n",
    "                    sample_data.append({\n",
    "                        'hour': date,\n",
    "                        'start_station_id': station_id,\n",
    "                        'ride_count': np.random.randint(0, 10),\n",
    "                        'temp': np.random.uniform(30, 90),\n",
    "                        'humidity': np.random.uniform(30, 90),\n",
    "                        'pressure': np.random.uniform(1000, 1020),\n",
    "                        'wind_speed': np.random.uniform(0, 20),\n",
    "                        'wind_deg': np.random.uniform(0, 360),\n",
    "                        'precipitation': np.random.uniform(0, 5)\n",
    "                    })\n",
    "            merged_data = pd.DataFrame(sample_data)\n",
    "            print(f\"Created sample data: {len(merged_data):,} records\")\n",
    "        \n",
    "        # Transform the data\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRANSFORMING DATA WITH SLIDING WINDOW\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        transformed_data = transform_ts_data_into_features_and_target(\n",
    "            df=merged_data,\n",
    "            feature_col=\"ride_count\",\n",
    "            rides_window_size=672,  # 28 days\n",
    "            weather_window_size=3,   # 3 hours\n",
    "            step_size=1\n",
    "        )\n",
    "        \n",
    "        # Store the transformed data\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STORING TRANSFORMED DATA\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        final_data = store_transformed_data(transformed_data)\n",
    "        \n",
    "        # Analyze the results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        analyze_transformed_data(final_data)\n",
    "        \n",
    "        print(\"\\nðŸŽ‰ TRANSFORMATION COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"The dataset is now ready for machine learning model training.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in transformation pipeline: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nCode execution completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ef9de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
