{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d9065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Blue Bikes Data Transformation Pipeline\n",
      "==================================================\n",
      "Loading processed data...\n",
      "Loaded data: 4,950,745 records\n",
      "Columns: ['hour', 'start_station_id', 'ride_count', 'lat_round', 'lon_round', 'temp', 'humidity', 'pressure', 'wind_speed', 'wind_deg', 'precipitation']\n",
      "\n",
      "==================================================\n",
      "TRANSFORMING DATA WITH SLIDING WINDOW\n",
      "==================================================\n",
      "Starting sliding window transformation...\n",
      "Rides window size: 672 (28 days)\n",
      "Weather window size: 3 hours\n",
      "Step size: 1\n",
      "Number of stations after filtering: 50\n",
      "Processing 50 stations...\n",
      "Processed 10 stations...\n",
      "Processed 20 stations...\n",
      "Processed 30 stations...\n",
      "Processed 40 stations...\n",
      "Processed 50 stations...\n",
      "Transformation complete!\n",
      "Final dataset: 404584 records\n",
      "Number of features: 681\n",
      "  - Ride features: 672\n",
      "  - Weather features: 9\n",
      "\n",
      "==================================================\n",
      "STORING TRANSFORMED DATA\n",
      "==================================================\n",
      "Stored transformed data to: ../data/transformed/transformed_features_and_target_top50.parquet\n",
      "Dataset shape: (404584, 684)\n",
      "\n",
      "==================================================\n",
      "ANALYSIS\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "TRANSFORMED DATA ANALYSIS\n",
      "============================================================\n",
      "Total records: 404,584\n",
      "Number of stations: 50\n",
      "Time range: 2024-10-28 22:00:00 to 2025-09-30 23:00:00\n",
      "\n",
      "Feature breakdown:\n",
      "  Ride features: 672\n",
      "  Weather features: 9\n",
      "  Total features: 681\n",
      "Missing values: 3641256\n",
      "\n",
      "Target statistics:\n",
      "  Min: 0\n",
      "  Max: 98\n",
      "  Mean: 3.49\n",
      "  Std: 4.74\n",
      "\n",
      "Sample of transformed data:\n",
      "   target start_station_id                hour  rides_t-672  rides_t-671  \\\n",
      "0      13           A32002 2024-10-28 22:00:00          0.0          0.0   \n",
      "1       3           A32002 2024-10-28 23:00:00          0.0          0.0   \n",
      "2       1           A32002 2024-10-29 00:00:00          0.0          3.0   \n",
      "3       1           A32002 2024-10-29 01:00:00          3.0          1.0   \n",
      "4       0           A32002 2024-10-29 02:00:00          1.0          0.0   \n",
      "\n",
      "   rides_t-670  rides_t-669  rides_t-668  \n",
      "0          0.0          3.0          1.0  \n",
      "1          3.0          1.0          0.0  \n",
      "2          1.0          0.0          0.0  \n",
      "3          0.0          0.0          0.0  \n",
      "4          0.0          0.0          3.0  \n",
      "\n",
      "ðŸŽ‰ TRANSFORMATION COMPLETED SUCCESSFULLY!\n",
      "The dataset is now ready for machine learning model training.\n",
      "\n",
      "Code execution completed!\n"
     ]
    }
   ],
   "source": [
    "# MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Blue Bikes Data Transformation Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Load your merged data (adjust the path as needed)\n",
    "        processed_file = PROCESSED_DIR / \"rides_weather_combined.parquet\"\n",
    "        \n",
    "        # Load and validate the data\n",
    "        merged_data, weather_columns = load_and_validate_merged_data(processed_file)\n",
    "        \n",
    "        print(f\"\\nâœ“ Successfully loaded and validated data\")\n",
    "        print(f\"âœ“ Using weather columns: {weather_columns}\")\n",
    "        \n",
    "        # Transform the data with actual weather columns\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRANSFORMING DATA WITH SLIDING WINDOW\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        transformed_data = transform_ts_data_into_features_and_target(\n",
    "            df=merged_data,\n",
    "            feature_col=\"ride_count\",\n",
    "            weather_cols=weather_columns,  # Use detected weather columns\n",
    "            rides_window_size=672,  # 28 days\n",
    "            weather_window_size=3,   # 3 hours\n",
    "            step_size=1\n",
    "        )\n",
    "        \n",
    "        # Verify transformation quality\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"VERIFYING TRANSFORMATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Check for NaN values in transformed data\n",
    "        null_summary = transformed_data.isnull().sum()\n",
    "        if null_summary.sum() > 0:\n",
    "            print(\"\\nâš ï¸  Warning: Null values found in transformed data:\")\n",
    "            print(null_summary[null_summary > 0])\n",
    "        else:\n",
    "            print(\"âœ“ No null values in transformed data!\")\n",
    "        \n",
    "        # Store the transformed data\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STORING TRANSFORMED DATA\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        final_data = store_transformed_data(transformed_data)\n",
    "        \n",
    "        # Analyze the results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        analyze_transformed_data(final_data)\n",
    "        \n",
    "        print(\"\\nðŸŽ‰ TRANSFORMATION COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"The dataset is now ready for machine learning model training.\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ File not found: {e}\")\n",
    "        print(\"\\nPlease ensure you have:\")\n",
    "        print(\"1. Run 03_process_and_save_data.ipynb first\")\n",
    "        print(\"2. The merged file exists at: data/processed/rides_weather_combined.parquet\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in transformation pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "print(\"\\nCode execution completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "9lx0zd5i02u",
   "source": "import sys\nimport os\n\n# Add the parent directory to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6zn89poqsgc",
   "source": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom typing import List, Tuple\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom src.config import RAW_DATA_DIR, PROCESSED_DATA_DIR, TRANSFORMED_DATA_DIR\n\n# Set up directories\nPROCESSED_DIR = PROCESSED_DATA_DIR\nTRANSFORMED_DIR = TRANSFORMED_DATA_DIR\n\nprint(f\"Processed directory: {PROCESSED_DIR}\")\nprint(f\"Transformed directory: {TRANSFORMED_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ef9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41521f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_merged_data(processed_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the merged rides+weather data and perform comprehensive validation\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"LOADING AND VALIDATING MERGED DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not processed_file.exists():\n",
    "        raise FileNotFoundError(f\"Processed file not found: {processed_file}\")\n",
    "    \n",
    "    print(f\"\\nLoading: {processed_file.name}\")\n",
    "    df = pd.read_parquet(processed_file)\n",
    "    \n",
    "    print(f\"Loaded {len(df):,} records\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Data types:\\n{df.dtypes}\\n\")\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    df['hour'] = pd.to_datetime(df['hour'])\n",
    "    df['start_station_id'] = df['start_station_id'].astype(str)\n",
    "    df['ride_count'] = pd.to_numeric(df['ride_count'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Identify weather columns\n",
    "    weather_cols = [\"temp\", \"humidity\", \"pressure\", \"wind_speed\", \"wind_deg\", \"precipitation\"]\n",
    "    available_weather_cols = [col for col in weather_cols if col in df.columns]\n",
    "    \n",
    "    print(f\"Available weather columns: {available_weather_cols}\")\n",
    "    \n",
    "    # Convert weather columns to numeric\n",
    "    for col in available_weather_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Validate and fill weather data\n",
    "    df = validate_and_fill_weather(df)\n",
    "    \n",
    "    # Final validation\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"FINAL DATA VALIDATION\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Stations: {df['start_station_id'].nunique()}\")\n",
    "    print(f\"Date range: {df['hour'].min()} to {df['hour'].max()}\")\n",
    "    print(f\"Ride count - Min: {df['ride_count'].min()}, Max: {df['ride_count'].max()}, Mean: {df['ride_count'].mean():.2f}\")\n",
    "    \n",
    "    # Check for any remaining null values\n",
    "    null_counts = df.isnull().sum()\n",
    "    if null_counts.sum() > 0:\n",
    "        print(f\"\\nâš ï¸  Remaining null values:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "    else:\n",
    "        print(\"\\nâœ“ No null values remaining!\")\n",
    "    \n",
    "    return df, available_weather_cols"
   ]
  },
  {
   "cell_type": "code",
   "id": "se9wzdupb58",
   "source": "def transform_ts_data_into_features_and_target(\n    df: pd.DataFrame,\n    feature_col: str,\n    weather_cols: List[str],\n    rides_window_size: int = 672,  # 28 days * 24 hours\n    weather_window_size: int = 3,  # 3 hours of weather history\n    step_size: int = 1\n) -> pd.DataFrame:\n    \"\"\"\n    Transform time series data into features and target for machine learning.\n    Creates lagged features from ride counts and weather data.\n    \n    Parameters:\n    - df: DataFrame with hourly data per station\n    - feature_col: Column name for the target variable (e.g., 'ride_count')\n    - weather_cols: List of weather column names to include\n    - rides_window_size: Number of historical ride hours to include as features\n    - weather_window_size: Number of historical weather hours to include as features\n    - step_size: Step size for sliding window\n    \n    Returns:\n    - DataFrame with transformed features and target\n    \"\"\"\n    print(\"Starting sliding window transformation...\")\n    print(f\"Rides window size: {rides_window_size} (28 days)\")\n    print(f\"Weather window size: {weather_window_size} hours\")\n    print(f\"Step size: {step_size}\")\n    \n    # Ensure data is sorted by station and hour\n    df = df.sort_values([\"start_station_id\", \"hour\"]).reset_index(drop=True)\n    \n    # Get unique stations and filter stations with enough data\n    stations = df[\"start_station_id\"].unique()\n    min_required_samples = rides_window_size + 100  # Need enough history\n    \n    # Filter stations with sufficient data\n    station_counts = df.groupby(\"start_station_id\").size()\n    valid_stations = station_counts[station_counts >= min_required_samples].index.tolist()\n    \n    # Limit to top 50 stations by ride count for faster processing\n    station_totals = df.groupby(\"start_station_id\")[feature_col].sum().sort_values(ascending=False)\n    valid_stations = [s for s in station_totals.head(50).index if s in valid_stations]\n    \n    print(f\"Number of stations after filtering: {len(valid_stations)}\")\n    \n    all_transformed_data = []\n    \n    print(f\"Processing {len(valid_stations)} stations...\")\n    \n    for idx, station in enumerate(valid_stations):\n        if (idx + 1) % 10 == 0:\n            print(f\"Processed {idx + 1} stations...\")\n        \n        # Get data for this station\n        station_data = df[df[\"start_station_id\"] == station].reset_index(drop=True)\n        \n        # Extract ride counts and weather features\n        ride_values = station_data[feature_col].values\n        weather_values = station_data[weather_cols].values if weather_cols else None\n        hours = station_data[\"hour\"].values\n        \n        # Create sliding window features\n        for i in range(rides_window_size, len(station_data), step_size):\n            # Target is the current hour\n            target = ride_values[i]\n            \n            # Features are historical rides\n            ride_features = ride_values[i - rides_window_size:i]\n            \n            # Add weather features (last N hours)\n            if weather_values is not None and weather_window_size > 0:\n                weather_features = weather_values[max(0, i - weather_window_size):i].flatten()\n            else:\n                weather_features = []\n            \n            # Combine all features\n            features = {\n                'target': target,\n                'start_station_id': station,\n                'hour': hours[i]\n            }\n            \n            # Add ride lag features\n            for j, val in enumerate(ride_features):\n                features[f'rides_t-{rides_window_size - j}'] = val\n            \n            # Add weather lag features\n            if len(weather_features) > 0:\n                for j, val in enumerate(weather_features):\n                    weather_idx = j // len(weather_cols)\n                    col_idx = j % len(weather_cols)\n                    features[f'weather_{weather_cols[col_idx]}_t-{weather_window_size - weather_idx - 1}'] = val\n            \n            all_transformed_data.append(features)\n    \n    # Create DataFrame\n    transformed_df = pd.DataFrame(all_transformed_data)\n    \n    print(\"\\nTransformation complete!\")\n    print(f\"Final dataset: {len(transformed_df)} records\")\n    \n    # Count features\n    ride_features = [col for col in transformed_df.columns if col.startswith('rides_t-')]\n    weather_features = [col for col in transformed_df.columns if col.startswith('weather_')]\n    \n    print(f\"Number of features: {len(ride_features) + len(weather_features)}\")\n    print(f\"  - Ride features: {len(ride_features)}\")\n    print(f\"  - Weather features: {len(weather_features)}\")\n    \n    return transformed_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4br1bemg7ah",
   "source": "def store_transformed_data(transformed_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Store the transformed data to the transformed directory\n    \"\"\"\n    # Ensure the transformed directory exists\n    TRANSFORMED_DIR.mkdir(parents=True, exist_ok=True)\n    \n    # Save to parquet\n    output_file = TRANSFORMED_DIR / \"transformed_features_and_target_top50.parquet\"\n    transformed_df.to_parquet(output_file, index=False)\n    \n    print(f\"Stored transformed data to: {output_file}\")\n    print(f\"Dataset shape: {transformed_df.shape}\")\n    \n    return transformed_df\n\n\ndef analyze_transformed_data(df: pd.DataFrame):\n    \"\"\"\n    Analyze the transformed dataset and print summary statistics\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"TRANSFORMED DATA ANALYSIS\")\n    print(\"=\"*60)\n    \n    print(f\"Total records: {len(df):,}\")\n    print(f\"Number of stations: {df['start_station_id'].nunique()}\")\n    print(f\"Time range: {df['hour'].min()} to {df['hour'].max()}\")\n    \n    # Count features\n    ride_features = [col for col in df.columns if col.startswith('rides_t-')]\n    weather_features = [col for col in df.columns if col.startswith('weather_')]\n    \n    print(f\"\\nFeature breakdown:\")\n    print(f\"  Ride features: {len(ride_features)}\")\n    print(f\"  Weather features: {len(weather_features)}\")\n    print(f\"  Total features: {len(ride_features) + len(weather_features)}\")\n    \n    # Check for missing values\n    missing_count = df.isnull().sum().sum()\n    print(f\"Missing values: {missing_count}\")\n    \n    # Target statistics\n    print(f\"\\nTarget statistics:\")\n    print(f\"  Min: {df['target'].min()}\")\n    print(f\"  Max: {df['target'].max()}\")\n    print(f\"  Mean: {df['target'].mean():.2f}\")\n    print(f\"  Std: {df['target'].std():.2f}\")\n    \n    # Display sample\n    print(f\"\\nSample of transformed data:\")\n    display_cols = ['target', 'start_station_id', 'hour'] + ride_features[:5]\n    print(df[display_cols].head())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "g4kpytbmdrn",
   "source": "def validate_and_fill_weather(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Validate and fill missing weather data using forward fill and station means\n    \"\"\"\n    weather_cols = [\"temp\", \"humidity\", \"pressure\", \"wind_speed\", \"wind_deg\", \"precipitation\"]\n    available_weather_cols = [col for col in weather_cols if col in df.columns]\n    \n    print(f\"Validating weather columns: {available_weather_cols}\")\n    \n    for col in available_weather_cols:\n        # Count missing before filling\n        missing_before = df[col].isnull().sum()\n        \n        if missing_before > 0:\n            # Sort by station and hour for proper forward fill\n            df = df.sort_values([\"start_station_id\", \"hour\"]).reset_index(drop=True)\n            \n            # Forward fill by station group\n            df[col] = df.groupby(\"start_station_id\")[col].transform(\n                lambda x: x.fillna(method='ffill').fillna(method='bfill')\n            )\n            \n            # Fill remaining with station mean\n            df[col] = df.groupby(\"start_station_id\")[col].transform(\n                lambda x: x.fillna(x.mean())\n            )\n            \n            # Fill any remaining NaN with global mean\n            df[col] = df[col].fillna(df[col].mean())\n            \n            missing_after = df[col].isnull().sum()\n            print(f\"  {col}: filled {missing_before - missing_after} missing values\")\n    \n    return df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}